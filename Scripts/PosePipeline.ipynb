{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7030219d-13a3-4037-a21e-52be4c5dc7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 13:24:56.208177: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-26 13:24:56.223050: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748280296.240309  166816 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748280296.245684  166816 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-26 13:24:56.263835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748280298.795650  166816 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 28448 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:73:00.0, compute capability: 8.6\n",
      "2025-05-26 13:26:19.052707: W external/xla/xla/service/gpu/nvptx_compiler.cc:765] The NVIDIA driver's CUDA version is 12.4 which is older than the ptxas CUDA version (12.5.82). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "# only use the first GPU if there are multiple\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "# limit jax and TF from consuming all GPU memory\n",
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "\n",
    "# List available GPUs in TensorFlow\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "\n",
    "# load metrab model \n",
    "model = hub.load('https://bit.ly/metrabs_l')  # Takes about 3 minutes\n",
    "skeleton = 'mpi_inf_3dhp_17'\n",
    "\n",
    "# load gait transformer model \n",
    "from gait_transformer.gait_phase_transformer import load_default_model, get_gait_phase_stride_transformer, gait_phase_stride_inference\n",
    "from gait_transformer.gait_phase_kalman import gait_kalman_smoother, compute_phases, get_event_times\n",
    "from tensorflow import keras\n",
    "\n",
    "pos_divider = 2\n",
    "transformer_model = load_default_model(pos_divider=pos_divider)\n",
    "\n",
    "# change joint order \n",
    "joint_names = np.array(['htop', 'neck', 'rsho', 'relb', 'rwri', 'lsho', 'lelb', 'lwri',\n",
    "       'rhip', 'rkne', 'rank', 'lhip', 'lkne', 'lank', 'pelv', 'spin',\n",
    "       'head'])\n",
    "# this is the order of joints from the Gast-NET algorithm that the gait transformer was originally trained on\n",
    "expected_order = ['pelv', 'rhip', 'rkne', 'rank', 'lhip', 'lkne', 'lank', 'spin', 'neck', 'head', 'htop', 'lsho', 'lelb', 'lwri', 'rsho', 'relb', 'rwri']\n",
    "expected_order_idx = np.array([joint_names.tolist().index(j) for j in expected_order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53941ab7-64d7-4486-a4b1-7a0d0bb3dd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mirrored video saved as ../Downloads/Results/mirror_video/demo.mp4\n",
      "Step 1: Extracting pose keypoints from video files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing demo.mp4: 0it [00:00, ?it/s]2025-05-26 13:30:08.725586: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n",
      "I0000 00:00:1748280611.477897  382166 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748280613.446371  382124 cuda_solvers.cc:178] Creating GpuSolver handles for stream 0xfead710\n",
      "Processing demo.mp4: 34it [00:55,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed demo.mp4 and saved keypoints to demo.npy\n",
      "Step 2: Running Gait Transformer model on extracted keypoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: ../Downloads/Results/JSON/demo_gait_events_L60.json\n",
      "Step 3: Arranging gait events and exporting to Excel...\n",
      "Saved arranged gait events to ../Downloads/Results/gaitevents_video.xlsx\n",
      "✅ Full gait processing pipeline complete.\n",
      "Step 1: Extracting pose keypoints from video files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing demo.mp4: 34it [00:26,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed demo.mp4 and saved keypoints to demo.npy\n",
      "Step 2: Running Gait Transformer model on extracted keypoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: ../Downloads/Results/mirror_JSON/demo_gait_events_L60.json\n",
      "Step 3: Arranging gait events and exporting to Excel...\n",
      "Saved arranged gait events to ../Downloads/Results/gaitevents_mirror.xlsx\n",
      "✅ Full gait processing pipeline complete.\n",
      "Excel file 'feature_results.xlsx' has been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Main code \n",
    "\n",
    "# set up\n",
    "height_cm = 166 \n",
    "# Set up directories\n",
    "input_dir = \"./DEMO\"\n",
    "\n",
    "Output_dir = os.path.join(os.path.dirname(input_dir), \"Results\")\n",
    "output_dir = os.path.join(os.path.dirname(input_dir), \"Results\", \"mirror_video\")\n",
    "# Make sure the directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List all .mp4 files in input folder\n",
    "file_names = [f for f in os.listdir(input_dir) if f.endswith(\".mp4\")]\n",
    "file_names.sort(key=extract_parts)\n",
    "\n",
    "# Track already processed files\n",
    "processed_files = {os.path.splitext(f)[0] for f in os.listdir(output_dir) if f.endswith(\".mp4\")}\n",
    "\n",
    "# Loop through and mirror videos\n",
    "for file_name in file_names:\n",
    "    video_name = os.path.splitext(file_name)[0]\n",
    "    if video_name in processed_files:\n",
    "        print(f\"Skipping {file_name} — already processed.\")\n",
    "        continue\n",
    "\n",
    "    input_path = os.path.join(input_dir, file_name)\n",
    "    output_path = os.path.join(output_dir, file_name)\n",
    "    mirror_video(input_path, output_path)\n",
    "\n",
    "\n",
    "# Run video processing \n",
    "run_full_gait_processing_pipeline(\n",
    "    video_directory=input_dir,\n",
    "    keypoints_output_dir=os.path.join(Output_dir, \"keypoints\"),\n",
    "    gait_json_output_dir=os.path.join(Output_dir, \"JSON\"),\n",
    "    final_excel_path=os.path.join(Output_dir, \"gaitevents_video.xlsx\"),\n",
    "    model=model,\n",
    "    skeleton=skeleton,\n",
    "    video_reader=video_reader,\n",
    "    transformer_model=transformer_model,\n",
    "    height_mm=height_cm * 10,\n",
    "    expected_order_idx=expected_order_idx,\n",
    "    extract_parts=extract_parts,\n",
    "    convert_to_int=convert_to_int,\n",
    "    shift_invalid_rows=shift_invalid_rows\n",
    ")\n",
    "\n",
    "\n",
    "run_full_gait_processing_pipeline(\n",
    "    video_directory=output_dir,\n",
    "    keypoints_output_dir=os.path.join(Output_dir, \"mirror_keypoints\"),\n",
    "    gait_json_output_dir=os.path.join(Output_dir, \"mirror_JSON\"),\n",
    "    final_excel_path=os.path.join(Output_dir, \"gaitevents_mirror.xlsx\"),\n",
    "    model=model,\n",
    "    skeleton=skeleton,\n",
    "    video_reader=video_reader,\n",
    "    transformer_model=transformer_model,\n",
    "    height_mm=height_cm * 10,\n",
    "    expected_order_idx=expected_order_idx,\n",
    "    extract_parts=extract_parts,\n",
    "    convert_to_int=convert_to_int,\n",
    "    shift_invalid_rows=shift_invalid_rows\n",
    ")\n",
    "\n",
    "\n",
    "# Run feature extraction \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load your file\n",
    "df_video = pd.read_excel(os.path.join(Output_dir, \"gaitevents_video.xlsx\"))\n",
    "df_mirror = pd.read_excel(os.path.join(Output_dir, \"gaitevents_mirror.xlsx\"))\n",
    "\n",
    "# filter \n",
    "df_video_filtered = filter_numeric_columns(df_video, threshold=12)\n",
    "df_mirror_filtered = filter_numeric_columns(df_mirror, threshold=12)\n",
    "\n",
    "\n",
    "df_video_results = analyze_gait_video_features(\n",
    "    df_video=df_video_filtered,\n",
    "    keypoints_dir=os.path.join(Output_dir, \"keypoints\"),\n",
    "    expected_order_idx=expected_order_idx,\n",
    "    extract_parts=extract_parts\n",
    ")\n",
    "\n",
    "\n",
    "df_mirror_results = analyze_gait_video_features(\n",
    "    df_video=df_mirror_filtered,\n",
    "    keypoints_dir=os.path.join(Output_dir, \"mirror_keypoints\"),\n",
    "    expected_order_idx=expected_order_idx,\n",
    "    extract_parts=extract_parts\n",
    ")\n",
    "\n",
    "\n",
    "## swap mirror file columns \n",
    "# Get the current column names\n",
    "columns = df_mirror_results.columns.tolist()\n",
    "# Define the index pairs to swap\n",
    "swap_pairs = [(8,9), (10, 11), (12, 13), (14,15)]\n",
    "# Swap the column names\n",
    "for i, j in swap_pairs:\n",
    "    columns[i], columns[j] = columns[j], columns[i]\n",
    "# Assign the new column names back\n",
    "df_mirror_results.columns = columns\n",
    "\n",
    "# average \n",
    "# Merge on trial name (assumes the first column is trial name)\n",
    "merged = pd.merge(df_video_results, df_mirror_results, on='trial_name', suffixes=('_video', '_mirror'))\n",
    "# Identify numeric columns to average (exclude 'trial_name')\n",
    "numeric_cols = df_video_results.select_dtypes(include='number').columns\n",
    "# Create a new DataFrame for averaged results\n",
    "df_avg = pd.DataFrame()\n",
    "df_avg['trial_name'] = merged['trial_name']\n",
    "# Average each numeric column\n",
    "for col in numeric_cols:\n",
    "    col_video = f\"{col}_video\"\n",
    "    col_mirror = f\"{col}_mirror\"\n",
    "    df_avg[col] = (merged[col_video] + merged[col_mirror]) / 2\n",
    "\n",
    "\n",
    "# Save to Excel\n",
    "df_avg.to_excel(os.path.join(Output_dir, \"feature_results.xlsx\"), index=False)\n",
    "print(\"Excel file 'feature_results.xlsx' has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5c5e1f4-8cc3-43f3-b75f-8b4cf1300792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert values to integers\n",
    "def convert_to_int(data):\n",
    "    int_data = {}\n",
    "    for key, values in data.items():\n",
    "        int_data[key] = [int(value) for value in values]\n",
    "    return int_data\n",
    "\n",
    "def extract_parts(filename):\n",
    "    match = re.match(r'(Diz|Val)_(\\d+)_T(\\d+)', filename)\n",
    "    if match:\n",
    "        prefix = match.group(1)\n",
    "        part1 = int(match.group(2))\n",
    "        part2 = int(match.group(3))\n",
    "        # Use a sorting key that puts \"Diz\" (which is 0) before \"Val\" (which is 1)\n",
    "        prefix_order = 0 if prefix == \"Diz\" else 1\n",
    "        return (prefix_order, part1, part2)\n",
    "    return (float('inf'), float('inf'), float('inf'))  # Unmatched files last\n",
    "\n",
    "\n",
    "# read video and get keypoints \n",
    "def video_reader(filename: str, batch_size: int = 8, width=320):\n",
    "\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "\n",
    "    frames = []\n",
    "    while True:\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if ret is False:\n",
    "            \n",
    "            if len(frames) > 0:\n",
    "                frames = np.array(frames)\n",
    "                yield frames\n",
    "\n",
    "            cap.release()\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if width is not None:\n",
    "                # downsample to keep the aspect ratio and output the specified width\n",
    "                scale = width / frame.shape[1]\n",
    "                height = int(frame.shape[0] * scale)\n",
    "                frame = cv2.resize(frame, (width, height))\n",
    "            \n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) >= batch_size:\n",
    "                frames = np.array(frames)\n",
    "                yield frames\n",
    "                \n",
    "                frames = []\n",
    "\n",
    "    \n",
    "def shift_invalid_rows(data):\n",
    "    # Get the first column values\n",
    "    first_col = data[:, 0]\n",
    "    reference_value = first_col[0]  # First value as reference    # Shift rows where the first column value is greater than the reference value\n",
    "    for i in range(1, len(first_col)):\n",
    "        if first_col[i] > reference_value:\n",
    "            data[i] = np.hstack(([np.nan], data[i, :-1]))    \n",
    "    return data\n",
    "\n",
    "# mirror video function \n",
    "def mirror_video(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Mirrors a single video horizontally and saves the output.\n",
    "    \n",
    "    Parameters:\n",
    "        input_path (str): Path to the input video file.\n",
    "        output_path (str): Path to save the mirrored video.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open video: {input_path}\")\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        flipped_frame = cv2.flip(frame, 1)\n",
    "        out.write(flipped_frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Mirrored video saved as {output_path}\")\n",
    "\n",
    "\n",
    "# pose estimation function \n",
    "def process_video_for_keypoints(\n",
    "    file_name,\n",
    "    directory_path,\n",
    "    output_dir,\n",
    "    model,\n",
    "    skeleton,\n",
    "    video_reader,\n",
    "    processed_files=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a video to extract 3D pose keypoints if exactly one person is detected per frame.\n",
    "    Saves the pose data as a .npy file.\n",
    "\n",
    "    Parameters:\n",
    "        file_name (str): Name of the video file.\n",
    "        directory_path (str): Directory where the input video is located.\n",
    "        output_dir (str): Directory to save the output .npy file.\n",
    "        model: Pose detection model with .detect_poses_batched().\n",
    "        skeleton: Skeleton structure used by the model.\n",
    "        video_reader: Function to read video frames in batches.\n",
    "        processed_files (set): Set of already processed file base names (without extension).\n",
    "    \"\"\"\n",
    "    video_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "    if processed_files and video_name in processed_files:\n",
    "        print(f\"Skipping {file_name} — already processed.\")\n",
    "        return\n",
    "\n",
    "    video_filepath = os.path.join(directory_path, file_name)\n",
    "    vid = video_reader(video_filepath, width=None)\n",
    "\n",
    "    multiple_people_detected = False\n",
    "    nonvalid_pose_detected = False\n",
    "    accumulated = None\n",
    "\n",
    "    for i, frame_batch in tqdm(enumerate(vid), desc=f\"Processing {file_name}\"):\n",
    "        pred = model.detect_poses_batched(frame_batch, skeleton=skeleton)\n",
    "\n",
    "        if accumulated is None:\n",
    "            accumulated = pred\n",
    "        else:\n",
    "            for key in accumulated.keys():\n",
    "                accumulated[key] = tf.concat([accumulated[key], pred[key]], axis=0)\n",
    "\n",
    "        num_people = [p.shape[0] for p in accumulated['poses2d']]\n",
    "\n",
    "        if len(set(num_people)) > 1:\n",
    "            multiple_people_detected = True\n",
    "\n",
    "        if any(n == 0 for n in num_people):\n",
    "            nonvalid_pose_detected = True\n",
    "            break\n",
    "\n",
    "    if multiple_people_detected:\n",
    "        print(f\"2 - {file_name} has multiple people detected.\")\n",
    "        \n",
    "\n",
    "    if nonvalid_pose_detected:\n",
    "        print(f\"Skipping {file_name} — one or more frames have no detected person.\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    pose3d = np.array([p[0] for p in accumulated['poses3d']])\n",
    "    output_file_name = f\"{video_name}.npy\"\n",
    "    np.save(os.path.join(output_dir, output_file_name), pose3d)\n",
    "    print(f\"Processed {file_name} and saved keypoints to {output_file_name}\")\n",
    "\n",
    "\n",
    "# gait transformer function \n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from gait_transformer.gait_phase_transformer import gait_phase_stride_inference\n",
    "from gait_transformer.gait_phase_kalman import gait_kalman_smoother, get_event_times\n",
    "\n",
    "def process_gait_keypoints_to_json(\n",
    "    file_path,\n",
    "    output_directory,\n",
    "    transformer_model,\n",
    "    height,\n",
    "    expected_order_idx,\n",
    "    L=60,\n",
    "    pos_divider=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a 3D keypoints .npy file using the gait transformer model and saves gait events to JSON.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the .npy file containing 3D keypoints.\n",
    "        output_directory (str): Directory to save the resulting JSON file.\n",
    "        transformer_model: Loaded gait transformer model.\n",
    "        height (float): Subject height in mm (will be scaled internally).\n",
    "        expected_order_idx (np.array): Indices to reorder joints.\n",
    "        L (int): Window length for inference (default: 60).\n",
    "        pos_divider (int): Positional divider used in model loading (default: 2).\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    keypoints = np.load(file_path)\n",
    "\n",
    "    # Reorder, normalize and transform keypoints\n",
    "    keypoints = keypoints[:, expected_order_idx]\n",
    "    keypoints = keypoints / 1000.0          # mm → m\n",
    "    keypoints = keypoints - np.mean(keypoints, axis=1, keepdims=True)\n",
    "    keypoints = keypoints[:, :, [0, 2, 1]]  # reordering axes\n",
    "    keypoints[:, :, 2] *= -1                # flip z\n",
    "\n",
    "    # Run inference\n",
    "    phase, stride = gait_phase_stride_inference(keypoints, height, transformer_model, L * pos_divider)\n",
    "\n",
    "    # Kalman smoothing\n",
    "    phase_ordered = np.take(phase, [0, 4, 1, 5, 2, 6, 3, 7], axis=-1)\n",
    "    state, _, _ = gait_kalman_smoother(phase_ordered)\n",
    "    timestamps = np.arange(state.shape[0])\n",
    "    gait_event_dic = get_event_times(state, timestamps)\n",
    "\n",
    "    # Save result to JSON\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    output_path = os.path.join(output_directory, file_name.replace('.npy', f'_gait_events_L{L}.json'))\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({k: v.tolist() for k, v in gait_event_dic.items()}, f, indent=4)\n",
    "\n",
    "    print(f\"Processed and saved: {output_path}\")\n",
    "\n",
    "\n",
    "# arrange gait events function \n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def arrange_gait_events_to_excel(\n",
    "    directory_path,\n",
    "    output_excel_path,\n",
    "    extract_parts,\n",
    "    convert_to_int,\n",
    "    shift_invalid_rows\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes gait event JSON files, rearranges data, and saves to a structured Excel sheet.\n",
    "\n",
    "    Parameters:\n",
    "        directory_path (str): Directory containing gait event JSON files.\n",
    "        output_excel_path (str): Path to save the final Excel file.\n",
    "        extract_parts (function): Function used to sort files naturally.\n",
    "        convert_to_int (function): Function to convert JSON string data to integers.\n",
    "        shift_invalid_rows (function): Function to clean/adjust invalid data rows.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get list of L60 JSON files and sort them\n",
    "    file_names = [f for f in os.listdir(directory_path) if f.endswith('L60.json')]\n",
    "    file_names.sort(key=extract_parts)\n",
    "\n",
    "    dfs = []\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            int_data = convert_to_int(data)\n",
    "            df = pd.DataFrame.from_dict(int_data, orient='index')\n",
    "            dfs.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"No dataframes created. Exiting.\")\n",
    "        return\n",
    "\n",
    "    combined_df = pd.concat(dfs)\n",
    "    NUM_T = int(combined_df.shape[0] / 4)\n",
    "\n",
    "    # Add trial names\n",
    "    file_labels = [\"_\".join(f.split(\"_\")[:3]) for f in file_names for _ in range(4)]\n",
    "    combined_df.reset_index(inplace=True)\n",
    "    combined_df.insert(0, 'Trial', file_labels)\n",
    "    combined_df.rename(columns={'index': 'gait_event'}, inplace=True)\n",
    "\n",
    "    # Swap stride_L and stride_R\n",
    "    for k in range(NUM_T):\n",
    "        i1, i2 = 1 + 4 * k, 2 + 4 * k\n",
    "        combined_df.iloc[[i1, i2]] = combined_df.iloc[[i2, i1]].values\n",
    "\n",
    "    # Process data chunks\n",
    "    chunked_data = []\n",
    "    for i in range(0, len(combined_df), 4):\n",
    "        chunk = combined_df.iloc[i:i + 4]\n",
    "        processed = shift_invalid_rows(chunk.values[:, 2:])\n",
    "        chunked_data.append(processed)\n",
    "\n",
    "    final_data = np.vstack(chunked_data)\n",
    "    final_df = pd.DataFrame(np.hstack([combined_df[['Trial', 'gait_event']], final_data]),\n",
    "                            columns=['Trial', 'gait_event'] + combined_df.columns[2:].tolist())\n",
    "\n",
    "    final_df.to_excel(output_excel_path, index=False)\n",
    "    print(f\"Saved arranged gait events to {output_excel_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def run_full_gait_processing_pipeline(\n",
    "    video_directory,\n",
    "    keypoints_output_dir,\n",
    "    gait_json_output_dir,\n",
    "    final_excel_path,\n",
    "    model,\n",
    "    skeleton,\n",
    "    video_reader,\n",
    "    transformer_model,\n",
    "    height_mm,\n",
    "    expected_order_idx,\n",
    "    extract_parts,\n",
    "    convert_to_int,\n",
    "    shift_invalid_rows,\n",
    "    pos_divider=2,\n",
    "    L=60\n",
    "):\n",
    "    import os\n",
    "\n",
    "    # --- STEP 1: Pose Estimation from videos ---\n",
    "    print(\"Step 1: Extracting pose keypoints from video files...\")\n",
    "\n",
    "    # Make sure the keypoints output directory exists\n",
    "    os.makedirs(keypoints_output_dir, exist_ok=True)\n",
    "    \n",
    "    video_files = [f for f in os.listdir(video_directory) if f.endswith(\".mp4\")]\n",
    "    video_files.sort(key=extract_parts)\n",
    "    processed_files = {os.path.splitext(f)[0] for f in os.listdir(keypoints_output_dir) if f.endswith('.npy')}\n",
    "\n",
    "    for file_name in video_files:\n",
    "        process_video_for_keypoints(\n",
    "            file_name=file_name,\n",
    "            directory_path=video_directory,\n",
    "            output_dir=keypoints_output_dir,\n",
    "            model=model,\n",
    "            skeleton=skeleton,\n",
    "            video_reader=video_reader,\n",
    "            processed_files=processed_files\n",
    "        )\n",
    "\n",
    "    # --- STEP 2: Run Gait Transformer on keypoints ---\n",
    "    print(\"Step 2: Running Gait Transformer model on extracted keypoints...\")\n",
    "    keypoint_files = sorted(\n",
    "        [f for f in os.listdir(keypoints_output_dir) if f.endswith('.npy')],\n",
    "        key=extract_parts\n",
    "    )\n",
    "\n",
    "    for file_name in keypoint_files:\n",
    "        file_path = os.path.join(keypoints_output_dir, file_name)\n",
    "        process_gait_keypoints_to_json(\n",
    "            file_path=file_path,\n",
    "            output_directory=gait_json_output_dir,\n",
    "            transformer_model=transformer_model,\n",
    "            height=height_mm,\n",
    "            expected_order_idx=expected_order_idx,\n",
    "            L=L,\n",
    "            pos_divider=pos_divider\n",
    "        )\n",
    "\n",
    "    # --- STEP 3: Arrange Gait Events and Save to Excel ---\n",
    "    print(\"Step 3: Arranging gait events and exporting to Excel...\")\n",
    "    arrange_gait_events_to_excel(\n",
    "        directory_path=gait_json_output_dir,\n",
    "        output_excel_path=final_excel_path,\n",
    "        extract_parts=extract_parts,\n",
    "        convert_to_int=convert_to_int,\n",
    "        shift_invalid_rows=shift_invalid_rows\n",
    "    )\n",
    "\n",
    "    print(\"✅ Full gait processing pipeline complete.\")\n",
    "\n",
    "\n",
    "# filter function \n",
    "import pandas as pd\n",
    "def filter_numeric_columns(df: pd.DataFrame, threshold: float = 12) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters numeric columns in a DataFrame by keeping only values >= threshold.\n",
    "    Non-numeric columns are preserved and returned as-is.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with mixed types.\n",
    "        threshold (float): Minimum value to retain in numeric columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with numeric values below threshold set to NaN.\n",
    "    \"\"\"\n",
    "    df_numeric = df.select_dtypes(include='number')\n",
    "    df_filtered = df_numeric.where(df_numeric >= threshold)\n",
    "    df_non_numeric = df.select_dtypes(exclude='number')\n",
    "    df_combined = pd.concat([df_non_numeric, df_filtered], axis=1)\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "# get gait features function\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_gait_video_features(\n",
    "    df_video: pd.DataFrame,\n",
    "    keypoints_dir: str,\n",
    "    expected_order_idx: list,\n",
    "    fps: int = 60,\n",
    "    extract_parts=None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze spatiotemporal gait features from gait event timing and 3D keypoints.\n",
    "\n",
    "    Parameters:\n",
    "        df_video (pd.DataFrame): Gait event data organized in 4-row blocks (LHS, LTO, RHS, RTO).\n",
    "        keypoints_dir (str): Directory containing .npy keypoint files.\n",
    "        expected_order_idx (list): Joint reordering index.\n",
    "        fps (int): Frames per second.\n",
    "        output_path (str): Path to save the output Excel file.\n",
    "        extract_parts (function): Function for natural file sorting.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame of extracted gait features.\n",
    "    \"\"\"\n",
    "    # Pre-allocate lists\n",
    "    trial_name_list, avg_swing_left_list, avg_swing_right_list = [], [], []\n",
    "    avg_stance_left_list, avg_stance_right_list = [], []\n",
    "    avg_steptime_left_list, avg_steptime_right_list = [], []\n",
    "    avg_step_length_left_list, avg_step_length_right_list = [], []\n",
    "    cadence_list, avg_swing_list, avg_stance_list = [], [], []\n",
    "    avg_double_list, avg_velocity_list, avg_steplength_list = [], [], []\n",
    "    avg_steptime_list, correlation_list = [], []\n",
    "\n",
    "    files = sorted(os.listdir(keypoints_dir), key=extract_parts)\n",
    "    loop = len(df_video)\n",
    "\n",
    "    for m in range(loop // 4):\n",
    "        # Extract gait events\n",
    "        lhs, ltf = df_video.loc[4*m,0:], df_video.loc[4*m+1,0:]\n",
    "        rhs, rtf = df_video.loc[4*m+2,0:], df_video.loc[4*m+3,0:]\n",
    "\n",
    "        # Compute temporal phases\n",
    "        L_swing = (lhs - ltf).dropna()\n",
    "        L_stance = (ltf[1:].reset_index(drop=True) - lhs.reset_index(drop=True)).dropna()\n",
    "        R_swing = (rhs - rtf).dropna()\n",
    "        R_stance = (rtf[1:].reset_index(drop=True) - rhs.reset_index(drop=True)).dropna()\n",
    "        L_steptime = (lhs - rhs).dropna()\n",
    "        R_steptime = (rhs[1:].reset_index(drop=True) - lhs.reset_index(drop=True)).dropna()\n",
    "\n",
    "        # Combine phases\n",
    "        swing = np.concatenate([L_swing, R_swing])\n",
    "        stance = np.concatenate([L_stance, R_stance])\n",
    "        steptime = np.concatenate([L_steptime, R_steptime])\n",
    "        double = (rtf[1:].reset_index(drop=True) - lhs.reset_index(drop=True)).dropna().reset_index(drop=True)\n",
    "        double += (ltf - rhs).reset_index(drop=True).dropna().reset_index(drop=True)\n",
    "\n",
    "        # Averages (temporal)\n",
    "        avg_swing_left_list.append(np.mean(L_swing)/fps)\n",
    "        avg_swing_right_list.append(np.mean(R_swing)/fps)\n",
    "        avg_stance_left_list.append(np.mean(L_stance)/fps)\n",
    "        avg_stance_right_list.append(np.mean(R_stance)/fps)\n",
    "        avg_steptime_left_list.append(np.mean(L_steptime)/fps)\n",
    "        avg_steptime_right_list.append(np.mean(R_steptime)/fps)\n",
    "        avg_swing_list.append(np.mean(swing)/fps)\n",
    "        avg_stance_list.append(np.mean(stance)/fps)\n",
    "        avg_double_list.append(np.mean(double)/fps)\n",
    "        avg_steptime = np.mean(steptime)/fps\n",
    "        avg_steptime_list.append(avg_steptime)\n",
    "        cadence_list.append(60 / avg_steptime)\n",
    "\n",
    "        # Load keypoints\n",
    "        npy_file_path = os.path.join(keypoints_dir, files[m].split(\".\")[0] + '.npy')\n",
    "        keypoints = np.load(npy_file_path)\n",
    "        keypoints = keypoints[:, expected_order_idx] / 1000.0\n",
    "        keypoints[:, :, 1] *= -1  # Flip Y axis\n",
    "        z_hip = keypoints[:, 0, 2]\n",
    "\n",
    "        # Step lengths\n",
    "        lhs_int, rhs_int = lhs.dropna().astype(int), rhs.dropna().astype(int)\n",
    "        if (np.isnan(rhs.iloc[0])) and (not np.isnan(lhs.iloc[0])):\n",
    "            min_len = min(len(lhs_int)-1, len(rhs_int))\n",
    "            step_length_left = abs(z_hip[lhs_int.iloc[1:(min_len + 1)]] - z_hip[rhs_int.iloc[:min_len]])\n",
    "            step_length_right = abs(z_hip[rhs_int.iloc[:min_len]] - z_hip[lhs_int.iloc[:min_len]])\n",
    "        else:\n",
    "            min_len = min(len(lhs_int), len(rhs_int)-1)\n",
    "            step_length_left = abs(z_hip[lhs_int.iloc[:min_len]] - z_hip[rhs_int.iloc[:min_len]])\n",
    "            step_length_right = abs(z_hip[rhs_int.iloc[1:(min_len + 1)]] - z_hip[lhs_int.iloc[:min_len]])\n",
    "\n",
    "        step_length = np.concatenate([step_length_left, step_length_right])\n",
    "        sort_heelstrike = sorted(lhs_int.tolist() + rhs_int.tolist())\n",
    "        avg_velocity = abs((z_hip[sort_heelstrike[-1]] - z_hip[sort_heelstrike[0]]) / \n",
    "                           (sort_heelstrike[-1] - sort_heelstrike[0]) * fps)\n",
    "\n",
    "        # Arm swing symmetry\n",
    "        keypoints_centered = keypoints - keypoints[:, 0:1]\n",
    "        correlation = np.corrcoef(\n",
    "            keypoints_centered[:, 15, 2],  # lelb\n",
    "            keypoints_centered[:, 12, 2]   # relb\n",
    "        )[0, 1]\n",
    "\n",
    "        # Append spatial metrics\n",
    "        avg_step_length_left_list.append(np.mean(step_length_left))\n",
    "        avg_step_length_right_list.append(np.mean(step_length_right))\n",
    "        avg_steplength_list.append(np.mean(step_length))\n",
    "        avg_velocity_list.append(avg_velocity)\n",
    "        correlation_list.append(correlation)\n",
    "        trial_name_list.append(files[m].split('.')[0])\n",
    "\n",
    "    # Results to DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        \"trial_name\": trial_name_list,\n",
    "        \"avg_stancetime\": avg_stance_list,\n",
    "        \"avg_swingtime\": avg_swing_list,\n",
    "        \"avg_doublesupporttime\": avg_double_list,\n",
    "        \"avg_steptime\": avg_steptime_list,\n",
    "        \"avg_steplength\": avg_steplength_list,\n",
    "        \"avg_velocity\": avg_velocity_list,\n",
    "        \"avg_cadence\": cadence_list,\n",
    "        \"avg_stancetime_left\": avg_stance_left_list,\n",
    "        \"avg_stancetime_right\": avg_stance_right_list,\n",
    "        \"avg_swingtime_left\": avg_swing_left_list,\n",
    "        \"avg_swingtime_right\": avg_swing_right_list,\n",
    "        \"avg_steptime_left\": avg_steptime_left_list,\n",
    "        \"avg_steptime_right\": avg_steptime_right_list,\n",
    "        \"avg_steplength_left\": avg_step_length_left_list,\n",
    "        \"avg_steplength_right\": avg_step_length_right_list,\n",
    "        \"arm_swing_corr\": correlation_list\n",
    "    })\n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d0de40-a878-4a83-b022-b2c98426c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"../Downloads/Results/feature_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fadecce-fd0a-4b45-987d-1dddbb70a0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_name</th>\n",
       "      <th>avg_stancetime</th>\n",
       "      <th>avg_swingtime</th>\n",
       "      <th>avg_doublesupporttime</th>\n",
       "      <th>avg_steptime</th>\n",
       "      <th>avg_steplength</th>\n",
       "      <th>avg_velocity</th>\n",
       "      <th>avg_cadence</th>\n",
       "      <th>avg_stancetime_left</th>\n",
       "      <th>avg_stancetime_right</th>\n",
       "      <th>avg_swingtime_left</th>\n",
       "      <th>avg_swingtime_right</th>\n",
       "      <th>avg_steptime_left</th>\n",
       "      <th>avg_steptime_right</th>\n",
       "      <th>avg_steplength_left</th>\n",
       "      <th>avg_steplength_right</th>\n",
       "      <th>arm_swing_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>demo</td>\n",
       "      <td>0.677222</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.549802</td>\n",
       "      <td>0.825167</td>\n",
       "      <td>1.485678</td>\n",
       "      <td>109.13268</td>\n",
       "      <td>0.681944</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.4125</td>\n",
       "      <td>0.413889</td>\n",
       "      <td>0.542361</td>\n",
       "      <td>0.552778</td>\n",
       "      <td>0.789667</td>\n",
       "      <td>0.860667</td>\n",
       "      <td>-0.959253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trial_name  avg_stancetime  avg_swingtime  avg_doublesupporttime  \\\n",
       "0       demo        0.677222       0.409524               0.266667   \n",
       "\n",
       "   avg_steptime  avg_steplength  avg_velocity  avg_cadence  \\\n",
       "0      0.549802        0.825167      1.485678    109.13268   \n",
       "\n",
       "   avg_stancetime_left  avg_stancetime_right  avg_swingtime_left  \\\n",
       "0             0.681944              0.677778              0.4125   \n",
       "\n",
       "   avg_swingtime_right  avg_steptime_left  avg_steptime_right  \\\n",
       "0             0.413889           0.542361            0.552778   \n",
       "\n",
       "   avg_steplength_left  avg_steplength_right  arm_swing_corr  \n",
       "0             0.789667              0.860667       -0.959253  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15307a71-58c3-470c-998f-efde428b281a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
